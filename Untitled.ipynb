{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the DataFrame outside the loop\n",
    "df = pd.read_excel(\"input.xlsx\")\n",
    "\n",
    "# Create a folder to store text files\n",
    "folder_name = \"Articles_Txt_Files\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Loop\n",
    "for i in range(len(df[\"URL\"])):\n",
    "    try:\n",
    "        url = df[\"URL\"][i]\n",
    "        url_id = df[\"URL_ID\"][i]\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        print(f\"URL_ID of URL {i+1} is:\", df[\"URL_ID\"][i])\n",
    "\n",
    "        Article_Text = \"\"\n",
    "        h1 = soup.find(\"body\").find(\"h1\")\n",
    "        title = h1.text.strip()\n",
    "        if h1:\n",
    "            Article_Text += f\"Title: {title}\\n\\n\"\n",
    "\n",
    "        paragraphs = soup.find(\"article\").find_all(\"p\")\n",
    "    #     print(\"Text of the URL :\")\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned_text = paragraph.text.strip()\n",
    "            Article_Text += cleaned_text + \"\\n\"\n",
    "    #         print(cleaned_text)\n",
    "\n",
    "        # Save the article text in file\n",
    "        file_path = os.path.join(folde------------------------------------------------r_name, f\"{url_id}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(Article_Text)\n",
    "\n",
    "        print(f\"\\nArticle Text of the URL saved in file: {file_path}\\n\")\n",
    "        print(\"-----------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"URL_ID:\", url_id, \", Error:\", e)\n",
    "        print(\"Sorry, but the page you are looking for doesn't exist.\")\n",
    "        print(f\"Error occurred while processing URL {url}\\n\")\n",
    "        print(\"-------------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0972fd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# i=0\n",
    "# s = 10\n",
    "# sd[\"POSITIVE SCORE\"][i] = s\n",
    "# sd[\"NEGATIVE SCORE\"][i] = s\n",
    "# sd[\"POLARITY SCORE\"][i] = s\n",
    "# sd[\"SUBJECTIVITY SCORE\"][i] = s\n",
    "# sd[\"AVG SENTENCE LENGTH\"][i] = s\n",
    "# sd[\"PERCENTAGE OF COMPLEX WORDS\"][i] = s\n",
    "# sd[\"FOG INDEX\"][i] = s\n",
    "# sd[\"AVG NUMBER OF WORDS PER SENTENCE\"][i] = s\n",
    "# sd[\"COMPLEX WORD COUNT\"][i] = s\n",
    "# sd[\"WORD COUNT\"][i] = s\n",
    "# sd[\"SYLLABLE PER WORD\"][i] = s\n",
    "# sd[\"AVG NUMBER OF WORDS PER SENTENCE\"][i] = s\n",
    "\n",
    "# sd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e458404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PositiveScore = pd.read_csv(\"MasterDictionary\\positive-words.txt\")\n",
    "\n",
    "# for line in PositiveScore:\n",
    "#     # Check if the line contains 'POSITIVE SCORE'\n",
    "#     if 'POSITIVE SCORE' in line:\n",
    "#         # Split the line into key and value based on the separator (assuming ':' here)\n",
    "#         key, value = line.split(':')\n",
    "#         # Extract and clean the value (remove whitespace)\n",
    "#         positive_score = float(value.strip())  # Assuming the score is in float format\n",
    "#         break  # Exit the loop once the positive score is found\n",
    "\n",
    "# # Now, positive_score contains the positive score from the text variable\n",
    "# print(\"Positive Score:\", positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39d3467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\goura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\goura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text_dir = \"ArticlesText\"\n",
    "stopwords_dir = \"StopWords\"\n",
    "sentiment_dir = \"MasterDictionary\"\n",
    "\n",
    "# Load all stop words from the stopwords directory and store in the set variable\n",
    "stop_words = set()\n",
    "for file_name in os.listdir(stopwords_dir):\n",
    "    with open(os.path.join(stopwords_dir, file_name), 'r', encoding='ISO-8859-1') as f:\n",
    "        stop_words.update(set(f.read().splitlines()))\n",
    "\n",
    "# Load all text files from the directory and store in a list (docs)\n",
    "docs = []\n",
    "for file_name in os.listdir(text_dir):\n",
    "    file_path = os.path.join(text_dir, file_name)\n",
    "    if os.path.isdir(file_path):\n",
    "        continue  # Skip directories\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:  # Use UTF-8 encoding\n",
    "        text = f.read()\n",
    "        # Tokenize the given text file\n",
    "        words = word_tokenize(text)\n",
    "        # Remove the stop words from the tokens\n",
    "        filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "        # Add each filtered tokens of each file into a list\n",
    "        docs.append(filtered_text)\n",
    "\n",
    "# Store positive and negative words from the directory\n",
    "pos = set()\n",
    "neg = set()\n",
    "\n",
    "for file_name in os.listdir(sentiment_dir):\n",
    "    with open(os.path.join(sentiment_dir, file_name), 'r', encoding='ISO-8859-1') as f:\n",
    "        if file_name == 'positive-words.txt':\n",
    "            pos.update(f.read().splitlines())\n",
    "        else:\n",
    "            neg.update(f.read().splitlines())\n",
    "\n",
    "# Now collect the positive and negative words from each file\n",
    "# Calculate the scores from the positive and negative words \n",
    "positive_words = []\n",
    "negative_words = []\n",
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []\n",
    "\n",
    "# Iterate through the list of docs\n",
    "for doc in docs:\n",
    "    positive_words.append([word for word in doc if word.lower() in pos])\n",
    "    negative_words.append([word for word in doc if word.lower() in neg])\n",
    "    positive_score.append(len(positive_words[-1]))\n",
    "    negative_score.append(len(negative_words[-1]))\n",
    "    polarity_score.append((positive_score[-1] - negative_score[-1]) / ((positive_score[-1] + negative_score[-1]) + 0.000001))\n",
    "    subjectivity_score.append((positive_score[-1] + negative_score[-1]) / (len(doc) + 0.000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d2117a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\goura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords corpus if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "avg_sentence_length = []\n",
    "Percentage_of_Complex_words = []\n",
    "Fog_Index = []\n",
    "complex_word_count = []\n",
    "avg_syllable_word_count = []\n",
    "\n",
    "# Avoid variable name conflict by renaming\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "def measure(file):\n",
    "    if file.endswith('.ipynb_checkpoints'):\n",
    "        return None, None, None, None, None  # Skip the file\n",
    "    with open(os.path.join(text_dir, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    # remove punctuations \n",
    "    text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "    # split the given text file into sentences\n",
    "    sentences = text.split('.')\n",
    "    # total number of sentences in a file\n",
    "    num_sentences = len(sentences)\n",
    "    # total words in the file\n",
    "    words = [word for word in text.split() if word.lower() not in stop_words_set]\n",
    "    num_words = len(words)\n",
    "\n",
    "    # complex words having syllable count is greater than 2\n",
    "    # Complex words are words in the text that contain more than two syllables.\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "\n",
    "    # Syllable Count Per Word\n",
    "    total_syllables = sum(count_syllables(word) for word in words)\n",
    "\n",
    "    avg_sentence_len = num_words / num_sentences\n",
    "    avg_syllable_word_count = total_syllables / len(words)\n",
    "    Percent_Complex_words = len(complex_words) / num_words\n",
    "    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n",
    "\n",
    "    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words), avg_syllable_word_count\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    if word.endswith('es'):\n",
    "        word = word[:-2]\n",
    "    elif word.endswith('ed'):\n",
    "        word = word[:-2]\n",
    "    vowels = 'aeiou'\n",
    "    return sum(1 for letter in word if letter.lower() in vowels)\n",
    "\n",
    "# iterate through each file or doc\n",
    "for file in os.listdir(text_dir):\n",
    "    x, y, z, a, b = measure(file)\n",
    "    if x is not None:\n",
    "        avg_sentence_length.append(x)\n",
    "        Percentage_of_Complex_words.append(y)\n",
    "        Fog_Index.append(z)\n",
    "        complex_word_count.append(a)\n",
    "        avg_syllable_word_count.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0d276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\goura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords corpus if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "word_count = []\n",
    "average_word_length = []\n",
    "pp_count = []\n",
    "\n",
    "# Avoid variable name conflict by renaming\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "def cleaned_words(file):\n",
    "    with open(os.path.join(text_dir, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        words = [word for word in text.split() if word.lower() not in stopwords_set]\n",
    "        length = sum(len(word) for word in words)\n",
    "        average_word_length = length / len(words)\n",
    "    return len(words), average_word_length\n",
    "\n",
    "def count_personal_pronouns(file):\n",
    "    with open(os.path.join(text_dir, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "        count = 0\n",
    "        for pronoun in personal_pronouns:\n",
    "            count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text)) # \\b is used to match word boundaries\n",
    "    return count\n",
    "\n",
    "for file in os.listdir(text_dir):\n",
    "    if file.endswith('.ipynb_checkpoints'):\n",
    "        continue  # Skip directories\n",
    "    x, y = cleaned_words(file)\n",
    "    word_count.append(x)\n",
    "    average_word_length.append(y)\n",
    "    pp_count.append(count_personal_pronouns(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe90dced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_df = pd.read_excel('Output Data Structure.xlsx')\n",
    "\n",
    "# URLs with IDs 36 and 49 do not exist (404 error), so we are going to drop these rows from the table\n",
    "output_df.drop([36, 49], axis=0, inplace=True)\n",
    "\n",
    "# These are the required parameters \n",
    "variables = [positive_score,\n",
    "            negative_score,\n",
    "            polarity_score,\n",
    "            subjectivity_score,\n",
    "            avg_sentence_length,\n",
    "            Percentage_of_Complex_words,\n",
    "            Fog_Index,\n",
    "            avg_sentence_length,\n",
    "            complex_word_count,\n",
    "            word_count,\n",
    "            avg_syllable_word_count,\n",
    "            pp_count,\n",
    "            average_word_length]\n",
    "\n",
    "# Write the values to the dataframe\n",
    "for i, var in enumerate(variables):\n",
    "    output_df.iloc[:, i+2] = var\n",
    "\n",
    "# Now save the dataframe to disk\n",
    "output_df.to_csv('Output_Data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
